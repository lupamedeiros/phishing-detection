{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Data Segregation\n",
    "\n",
    "In this section we are to split our dataset into two blocks. The first one is going to be used for traning and stored as training_data.csv. The other will be reserved for testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Login to WandB\n",
    "\n",
    "Run:\n",
    "\n",
    "    wandb login --relogin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import os\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Data Segregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "# ratio used to split train and test data\n",
    "test_size = 0.20\n",
    "\n",
    "# seed used to reproduce purposes\n",
    "seed = 57\n",
    "\n",
    "# reference column to straitify the data\n",
    "stratify = \"status\"\n",
    "\n",
    "# name of the input artifact\n",
    "artifact_input_name = \"phishing-detection/preprocessed_data.csv:latest\"\n",
    "\n",
    "# type of the artifact\n",
    "artifact_type = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s %(message)s\",\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# reference for a logging obj\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# initiate wandb project\n",
    "run = wandb.init(project=\"phishing-detection\", job_type=\"split_data\")\n",
    "\n",
    "logger.info(\"Downloading and reading artifact\")\n",
    "artifact = run.use_artifact(artifact_input_name)\n",
    "artifact_path = artifact.file()\n",
    "df = pd.read_csv(artifact_path)\n",
    "\n",
    "# Split firstily in train/test, then we further divide the dataset to train and validation\n",
    "logger.info(\"Splitting data into train and test\")\n",
    "splits = {}\n",
    "splits[\"train\"], splits[\"test\"] = train_test_split(df,\n",
    "                                                   test_size=test_size,\n",
    "                                                   random_state=seed,\n",
    "                                                   stratify=df[stratify])\n",
    "\n",
    "# Save the artifacts. We use a temporary directory so we do not leave any trace behind\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    \n",
    "    for split, df in splits.items():\n",
    "\n",
    "        # MAke the artifact name from the name of the split plus the provided root\n",
    "        artifact_name = f\"{split}.csv\"\n",
    "\n",
    "        # Get the path on disk within the tempo directory\n",
    "        temp_path = os.path.join(tmp_dir, artifact_name)\n",
    "\n",
    "        logger.info(f\"Uploading the {split} dataset to {artifact_name}\")\n",
    "\n",
    "        # Save then upload to W&B\n",
    "        df.to_csv(temp_path,index=False)\n",
    "\n",
    "        artifact = wandb.Artifact(name=artifact_name,\n",
    "                                  type=artifact_type,\n",
    "                                  description=f\"{split} split of dataset {artifact_input_name}\")\n",
    "        artifact.add_file(temp_path)\n",
    "\n",
    "        logger.info(\"Logging artifact\")\n",
    "        run.log_artifact(artifact)\n",
    "\n",
    "        # This waits for the artifact to be upload to W&B. If you do not add\n",
    "        # this, te temp directory might be removed before W&B had a chance to \n",
    "        # upload the datasets, and upload might fail\n",
    "        artifact.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the run\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0516115c1436dd02cc9aaeb46289bc8de003b2cfd80475fe9086d29053953beb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('phishing-detection')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
